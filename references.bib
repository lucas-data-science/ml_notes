```bibtex
@book{bruce2020practical,
  added-at = {2022-04-27T09:18:25.000+0200},
  author = {Bruce, Peter and Bruce, Andrew and Gedeck, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2a286ae47ef254de7a2bad9821300e1aa/fbuckermann},
  edition = 2,
  interhash = {67707b95a81c1f41a6189d55f23c04e8},
  intrahash = {a286ae47ef254de7a2bad9821300e1aa},
  keywords = {lecture:additional lecture:data-science},
  publisher = {O’Reilly Media, Inc.,},
  timestamp = {2022-04-29T14:44:19.000+0200},
  title = {Practical Statistics for Data Scientists},
  url = {https://learning.oreilly.com/library/view/practical-statistics-for/9781492072935/},
  year = 2020
} 


@ARTICLE{Deng2017,
  author={Deng, Yue and Bao, Feng and Kong, Youyong and Ren, Zhiquan and Dai, Qionghai},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Deep Direct Reinforcement Learning for Financial Signal Representation and Trading}, 
  year={2017},
  volume={28},
  number={3},
  pages={653-664},
  keywords={Training;Robustness;Learning (artificial intelligence);Artificial neural networks;Feature extraction;Optimization;Signal representation;Deep learning (DL);financial signal processing;neural network (NN) for finance;reinforcement learning (RL)},
  doi={10.1109/TNNLS.2016.2522401}}

@article{Moody98,
author = {Moody, John and Wu, Lizhong and Liao, Yuansong and Saffell, Matthew},
title = {Performance functions and reinforcement learning for trading systems and portfolios},
journal = {Journal of Forecasting},
volume = {17},
number = {5-6},
pages = {441-470},
keywords = {trading systems, asset allocation, portfolio optimization, reinforcement learning, recurrent reinforcement learning, dynamic programming, on-line learning, recursive updating, Bernoulli utility, differential Sharpe ratio, transactions costs, state dependence, performance functions, recurrence},
doi = {https://doi.org/10.1002/(SICI)1099-131X(1998090)17:5/6<441::AID-FOR707>3.0.CO;2-\#},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-131X%281998090%2917%3A5/6%3C441%3A%3AAID-FOR707%3E3.0.CO%3B2-%23},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291099-131X%281998090%2917%3A5/6%3C441%3A%3AAID-FOR707%3E3.0.CO%3B2-%23},
abstract = {Abstract We propose to train trading systems and portfolios by optimizing objective functions that directly measure trading and investment performance. Rather than basing a trading system on forecasts or training via a supervised learning algorithm using labelled trading data, we train our systems using recurrent reinforcement learning (RRL) algorithms. The performance functions that we consider for reinforcement learning are profit or wealth, economic utility, the Sharpe ratio and our proposed differential Sharpe ratio. The trading and portfolio management systems require prior decisions as input in order to properly take into account the effects of transactions costs, market impact, and taxes. This temporal dependence on system state requires the use of reinforcement versions of standard recurrent learning algorithms. We present empirical results in controlled experiments that demonstrate the efficacy of some of our methods for optimizing trading systems and portfolios. For a long/short trader, we find that maximizing the differential Sharpe ratio yields more consistent results than maximizing profits, and that both methods outperform a trading system based on forecasts that minimize MSE. We find that portfolio traders trained to maximize the differential Sharpe ratio achieve better risk-adjusted returns than those trained to maximize profit. Finally, we provide simulation results for an S\&P 500/TBill asset allocation system that demonstrate the presence of out-of-sample predictability in the monthly S\&P 500 stock index for the 25 year period 1970 through 1994. Copyright © 1998 John Wiley \& Sons, Ltd.},
year = {1998}
}

@ARTICLE{Moody01,
  author={Moody, J. and Saffell, M.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Learning to trade via direct reinforcement}, 
  year={2001},
  volume={12},
  number={4},
  pages={875-889},
  abstract={We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision-making is viewed as a stochastic control problem, and strategies are discovered directly. We present an adaptive algorithm called recurrent reinforcement learning (RRL) for discovering investment policies. The need to build forecasting models is eliminated, and better trading performance is obtained. The direct reinforcement approach differs from dynamic programming and reinforcement algorithms such as TD-learning and Q-learning, which attempt to estimate a value function for the control problem. We find that the RRL direct reinforcement framework enables a simpler problem representation, avoids Bellman's curse of dimensionality and offers compelling advantages in efficiency. We demonstrate how direct reinforcement can be used to optimize risk-adjusted investment returns (including the differential Sharpe ratio), while accounting for the effects of transaction costs. In extensive simulation work using real financial data, we find that our approach based on RRL produces better trading strategies than systems utilizing Q-learning (a value function method). Real-world applications include an intra-daily currency trader and a monthly asset allocation system for the S&P 500 Stock Index and T-Bills.},
  keywords={Investments;Asset management;Optimization methods;Portfolios;Decision making;Stochastic processes;Adaptive algorithm;Learning;Predictive models;Dynamic programming},
  doi={10.1109/72.935097},
  ISSN={1941-0093},
  month={July},}


@misc{machinelearning2025,
      title={Modern applications of machine learning in quantum sciences}, 
      author={Anna Dawid and Julian Arnold and Borja Requena and Alexander Gresch and Marcin Płodzień and Kaelan Donatella and Kim A. Nicoli and Paolo Stornati and Rouven Koch and Miriam Büttner and Robert Okuła and Gorka Muñoz-Gil and Rodrigo A. Vargas-Hernández and Alba Cervera-Lierta and Juan Carrasquilla and Vedran Dunjko and Marylou Gabrié and Patrick Huembeli and Evert van Nieuwenburg and Filippo Vicentini and Lei Wang and Sebastian J. Wetzel and Giuseppe Carleo and Eliška Greplová and Roman Krems and Florian Marquardt and Michał Tomza and Maciej Lewenstein and Alexandre Dauphin},
      year={2025},
      eprint={2204.04198},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2204.04198}, 
}

