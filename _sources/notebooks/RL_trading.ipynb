{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bad3ea",
   "metadata": {},
   "source": [
    "# Fonte e Resumo do Estudo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc009d8",
   "metadata": {},
   "source": [
    "Esse capítulo é um estudo sobre o trabalho *Deep Direct Reinforcement Learning for Financial\n",
    "Signal Representation and Trading* de *Yue Deng, Feng Bao, Youyong Kong, Zhiquan Ren, and Qionghai Dai*, presente em {cite:p}`Deng2017`.\n",
    "Implementações do trabalho constam no endereço do Github: [https://github.com/Pandede/FDRNN](https://github.com/Pandede/FDRNN).\n",
    "\n",
    "Os autores começam O texto com a seguinte pergunta: É possível treinar um computador para superar traders experientes na negociação de ativos financeiros? Neste artigo, buscaram responder a esse desafio por meio da introdução de uma rede neural profunda recorrente para representação de sinais financeiros em tempo real e tomada de decisões de negociação. O modelo usado é inspirado em dois conceitos de aprendizagem relacionados à biologia: o aprendizado profundo (deep learning, DL) e o aprendizado por reforço (reinforcement learning, RL).\n",
    "\n",
    "No framework proposto, a parte de DL detecta automaticamente as condições dinâmicas do mercado para extrair características informativas. Em seguida, o módulo de RL interage com essas representações profundas e toma decisões de compra e venda visando maximizar os retornos em um ambiente desconhecido. O sistema de aprendizado é implementado em uma rede neural complexa que combina estruturas profundas e recorrentes. Para lidar com o problema de desaparecimento do gradiente durante o treinamento de redes profundas, eles proporam um método de retropropagação no tempo (backpropagation through time) consciente da tarefa. A robustez do sistema neural é verificada tanto no mercado de ações quanto no de contratos futuros de commodities, sob diversas condições de teste. \n",
    "\n",
    "A ideia aqui é replicar a lógica dos autores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae1221",
   "metadata": {},
   "source": [
    "# Introdução \n",
    "\n",
    "Segue tabela com siglas usadas e seus significados.\n",
    "\n",
    "| Sigla | Significado                             |\n",
    "|-------|------------------------------------------|\n",
    "| AE    | Autoencoder                              |\n",
    "| BPTT  | Backpropagation through time             |\n",
    "| DL    | Deep learning                            |\n",
    "| DNN   | Deep neural network                      |\n",
    "| DRL   | Direct reinforcement learning            |\n",
    "| DDR   | Deep direct reinforcement                |\n",
    "| FDDR  | Fuzzy deep direct reinforcement          |\n",
    "| RDNN  | Recurrent DNN                            |\n",
    "| RL    | Reinforcement learning                   |\n",
    "| NN    | Neural network                           |\n",
    "| SR    | Sharpe ratio                             |\n",
    "| TP    | Total profits                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac7714",
   "metadata": {},
   "source": [
    "Treinar agentes inteligentes para operar automaticamente no mercado financeiro é um tema antigo e amplamente debatido dentro da inteligência artificial moderna. Em essência, o processo de negociação pode ser descrito como um problema de tomada de decisão em tempo real que envolve dois passos fundamentais: compreender o estado atual do mercado e executar a melhor ação possível com base nessa leitura.\n",
    "\n",
    "Diferentemente de tarefas clássicas de aprendizado supervisionado — que contam com dados rotulados por especialistas —, a tomada de decisão dinâmica é mais desafiadora porque exige que o agente explore um ambiente desconhecido sem orientação direta, enquanto precisa agir corretamente em tempo real. Essa capacidade de aprender por conta própria é justamente o foco do aprendizado por reforço (RL), uma abordagem inspirada na forma como seres vivos aprendem a se comportar, com raízes teóricas na neurociência e no controle de comportamento.\n",
    "\n",
    "Do ponto de vista teórico, o RL já foi bem formalizado como um problema de controle ótimo estocástico. Na prática, seus sucessos são comprovados em várias áreas, como navegação de robôs, jogos Atari, e controle de helicópteros. Em alguns casos, os algoritmos de RL chegaram até a superar especialistas humanos em determinadas tarefas. Isso nos leva a uma pergunta importante: será que também é possível treinar um modelo de RL capaz de vencer traders experientes nos mercados financeiros?\n",
    "\n",
    "A resposta a essa pergunta é difícil e envolve dois grandes desafios:\n",
    "\n",
    "1. Como representar e resumir o ambiente financeiro?\n",
    "O mercado financeiro é um ambiente altamente volátil, com dados ruidosos, instáveis e imprevisíveis. Para tentar lidar com isso, analistas costumam usar indicadores técnicos como médias móveis ou estocásticos, com o objetivo de extrair padrões úteis. No entanto, esses indicadores têm limitações — por exemplo, a média móvel pode funcionar bem em tendências claras, mas fracassar em cenários de reversão de tendência.\n",
    "\n",
    "Isso levanta uma nova questão: em vez de depender de indicadores manuais, seria possível extrair automaticamente representações mais robustas diretamente dos dados?\n",
    "\n",
    "2. Como lidar com a execução dinâmica das ordens de compra e venda?\n",
    "Negociar não é apenas tomar decisões a cada instante — é preciso considerar fatores práticos, como custos de transação e “slippage” (desvios entre o preço esperado e o preço real de execução). Se o sistema ficar trocando de posição (entre compra e venda) com muita frequência, isso pode gerar prejuízos mesmo que as decisões estejam corretas em teoria.\n",
    "\n",
    "Além disso, o sistema precisa “lembrar” do que fez no passado: decisões anteriores e posições mantidas influenciam o que deve ser feito agora. Como incluir essa memória no modelo, sem complicar demais a arquitetura?\n",
    "\n",
    "Para enfrentar essas duas questões, os autores propõem uma arquitetura baseada em Redes Neurais Profundas Recorrentes (RDNN). Esse modelo é dividido em duas partes principais:\n",
    "\n",
    "Uma rede profunda (DNN), que aprende automaticamente a extrair características relevantes do mercado;\n",
    "\n",
    "Uma rede recorrente (RNN), que lida com a tomada de decisões no tempo, mantendo memória das ações passadas.\n",
    "\n",
    "Para melhorar ainda mais a robustez frente à incerteza do mercado, os autores incorporam conceitos de aprendizado fuzzy (fuzzy learning), que ajudam a lidar com imprecisões nos dados de entrada.\n",
    "\n",
    "Embora o aprendizado profundo (DL) já tenha se mostrado promissor em áreas como reconhecimento de imagem e fala, esta é a primeira vez, segundo os autores, que ele é implementado para desenvolver um sistema real de negociação financeira com aprendizado por reforço totalmente automatizado.\n",
    "\n",
    "O modelo completo resulta em uma rede neural bastante complexa, combinando profundidade e recorrência. Para treinar esse tipo de rede, é necessário “desenrolar” a parte recorrente no tempo, usando o método conhecido como Backpropagation Through Time (BPTT). No entanto, isso traz um problema: o desaparecimento do gradiente, que dificulta o aprendizado em redes muito profundas.\n",
    "\n",
    "Para contornar isso, os autores propõem um novo método de treinamento chamado BPTT consciente da tarefa (task-aware BPTT). Ele consiste em criar conexões virtuais entre a função objetivo final e camadas profundas da rede durante o treinamento, permitindo que essas camadas “enxerguem” diretamente o impacto de seus ajustes no resultado final — o que melhora a eficiência do aprendizado.\n",
    "\n",
    "O sistema proposto, chamado DDR (Deep Direct Reinforcement), é testado em mercados financeiros reais, usando contratos futuros de ações e de commodities. Os dados históricos desses ativos são utilizados para verificar a performance do modelo.\n",
    "\n",
    "Nas comparações realizadas, os autores mostram que o DDR (e sua versão fuzzy) é mais robusto que outros sistemas de negociação, sendo capaz de gerar lucros consistentes em diferentes condições de mercado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e2165",
   "metadata": {},
   "source": [
    "# Direct Deep Reinforcement Learning\n",
    "\n",
    "## A. Negociação por Reforço Direto (Direct Reinforcement Trading)\n",
    "\n",
    "Nesta seção, os autores revisam a estrutura de *aprendizado por reforço direto* (DRL - *Direct Reinforcement Learning*), originalmente proposta por {cite:p}`Moody98`. Eles mostram que esse tipo de DRL pode ser interpretado como uma **rede neural recorrente (RNN)** com apenas **uma camada**, o que a torna um modelo relativamente simples, mas ainda assim útil.\n",
    "\n",
    "O ambiente é modelado como uma sequência de preços de um ativo financeiro ao longo do tempo:\n",
    "\n",
    "$$\n",
    "p_1, p_2, \\dots, p_t, \\dots\n",
    "$$\n",
    "\n",
    "A variação de preço (ou retorno) entre dois instantes consecutivos é definida por:\n",
    "\n",
    "$$\n",
    "z_t = p_t - p_{t-1}\n",
    "$$\n",
    "\n",
    "Com base nas condições atuais do mercado, o modelo toma uma **decisão de negociação em tempo real** a cada instante de tempo $t$. Essa decisão, chamada **$\\delta_t$**, pode assumir os seguintes valores:\n",
    "\n",
    "- $1$: posição comprada (*long*)\n",
    "- $0$: posição neutra (*neutral*)\n",
    "- $-1$: posição vendida (*short*)\n",
    "\n",
    "O **lucro (ou prejuízo) no tempo $t$**, representado por $R_t$, é dado pela seguinte equação:\n",
    "\n",
    "$$\n",
    "R_t = \\delta_{t-1} \\cdot z_t - c \\cdot \\left| \\delta_t - \\delta_{t-1} \\right|\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "### Interpretação da fórmula:\n",
    "\n",
    "- O primeiro termo $\\delta_{t-1} \\cdot z_t$ representa o ganho (ou perda) obtido com a variação do mercado, **de acordo com a posição assumida no período anterior**.\n",
    "  - Exemplo: se você estava comprado ($\\delta_{t-1} = 1$) e o preço subiu ($z_t > 0$), você tem lucro.\n",
    "  - Se estava vendido ($\\delta_{t-1} = -1$) e o preço caiu ($z_t < 0$), também há lucro.\n",
    "  - Se estava neutro ($\\delta_{t-1} = 0$), esse termo é nulo.\n",
    "\n",
    "- O segundo termo $c \\cdot \\left| \\delta_t - \\delta_{t-1} \\right|$ representa o **custo de transação (transaction cost - TC)**. Ele é cobrado **apenas quando há mudança de posição** entre dois instantes consecutivos.\n",
    "  - Exemplo: mudar de neutro para comprado gera custo.\n",
    "  - Manter a mesma posição ($\\delta_t = \\delta_{t-1}$) **não gera custo**.\n",
    "\n",
    "O parâmetro $c$ representa esse custo, que pode incluir taxas da corretora, impostos, ou perdas por slippage (diferença entre o preço esperado e o executado). Esses custos são importantes para simular de forma realista os impactos de decisões frequentes de compra e venda no lucro final.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f4d98",
   "metadata": {},
   "source": [
    "## Exemplos de Ganho e Perda com a Fórmula de Reforço Direto\n",
    "\n",
    "A fórmula do lucro/prejuízo a cada instante de tempo $ t $ é:\n",
    "\n",
    "$$\n",
    "R_t = \\delta_{t-1} \\cdot z_t - c \\cdot |\\delta_t - \\delta_{t-1}|\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ \\delta_t \\in \\{-1, 0, 1\\} $: posição assumida (vendida, neutra ou comprada)\n",
    "- $ z_t = p_t - p_{t-1} $: variação de preço no período\n",
    "- $ c $: custo de transação (por exemplo, $ c = 0.5 $)\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 1: Lucro com posição comprada\n",
    "\n",
    "- $ p_{t-1} = 100 $\n",
    "- $ p_t = 103 $ → $ z_t = 3 $\n",
    "- $ \\delta_{t-1} = 1 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = 1 \\cdot 3 - 0.5 \\cdot |1 - 1| = 3 - 0 = \\boxed{3}\n",
    "$$\n",
    "\n",
    "**Lucro de 3 unidades**.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 2: Prejuízo com posição comprada\n",
    "\n",
    "- $ z_t = -2 $ (queda de preço)\n",
    "- $ \\delta_{t-1} = 1 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = 1 \\cdot (-2) - 0.5 \\cdot |1 - 1| = -2 - 0 = \\boxed{-2}\n",
    "$$\n",
    "\n",
    "**Prejuízo de 2 unidades**, sem custo de transação.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 3: Lucro com posição vendida\n",
    "\n",
    "- $ z_t = -4 $\n",
    "- $ \\delta_{t-1} = -1 $\n",
    "- $ \\delta_t = -1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = (-1) \\cdot (-4) - 0.5 \\cdot |-1 - (-1)| = 4 - 0 = \\boxed{4}\n",
    "$$\n",
    "\n",
    "**Lucro de 4 unidades**, pois o preço caiu e o agente estava vendido.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 4: Mudança de posição com custo, sem retorno imediato\n",
    "\n",
    "- $ z_t = 2 $\n",
    "- $ \\delta_{t-1} = 0 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = 0 \\cdot 2 - 0.5 \\cdot |1 - 0| = 0 - 0.5 = \\boxed{-0.5}\n",
    "$$\n",
    "\n",
    "***Perda de 0.5 unidades**, referente apenas ao custo de entrada.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 5: Mudança de posição com lucro subsequente\n",
    "\n",
    "#### No tempo $ t $:\n",
    "\n",
    "- $ z_t = 5 $\n",
    "- $ \\delta_{t-1} = 0 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = 0 \\cdot 5 - 0.5 \\cdot |1 - 0| = 0 - 0.5 = \\boxed{-0.5}\n",
    "$$\n",
    "\n",
    "#### No tempo $ t+1 $:\n",
    "\n",
    "- $ z_{t+1} = 3 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ \\delta_{t+1} = 1 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_{t+1} = 1 \\cdot 3 - 0.5 \\cdot |1 - 1| = 3 - 0 = \\boxed{3}\n",
    "$$\n",
    "\n",
    "**Resumo**: perdeu 0.5 na entrada, mas ganhou 3 depois → **Lucro líquido: 2.5**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8545dd",
   "metadata": {},
   "source": [
    "A função apresentada anteriormente (Equação 1) representa a **função de valor** típica usada em frameworks de *Deep Reinforcement Learning* (DRL). Essa função avalia o **lucro instantâneo** $ R_t $ em cada ponto do tempo.\n",
    "\n",
    "Com base nesses valores $ R_t $, é possível definir o valor acumulado ao longo de todo o período de treinamento como:\n",
    "\n",
    "$$\n",
    "\\max_{\\Theta} \\ U_T\\{R_1, R_2, \\dots, R_T \\ | \\ \\Theta\\}\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ U_T\\{\\cdot\\} $ é a função que **acumula os lucros/recompensas** entre os tempos $ t = 1 $ até $ T $\n",
    "- $ \\Theta $ é a política de negociação, ou seja, a estratégia usada para tomar decisões ao longo do tempo\n",
    "- O objetivo do treinamento é encontrar a política $ \\Theta $ que **maximiza** esse valor acumulado\n",
    "\n",
    "#### Recompensa acumulada mais simples: Total Profit (TP)\n",
    "\n",
    "A forma mais direta de acumular as recompensas é simplesmente somar os lucros em todos os períodos:\n",
    "\n",
    "$$\n",
    "U_T = \\sum_{t=1}^{T} R_t\n",
    "$$\n",
    "\n",
    "Essa soma representa o **lucro total (Total Profit – TP)** obtido no período completo de treinamento.\n",
    "\n",
    "#### Alternativas mais sofisticadas\n",
    "\n",
    "Embora o lucro total seja a forma mais intuitiva de recompensa, é possível utilizar **funções de recompensa mais complexas**, como:\n",
    "\n",
    "- Retorno ajustado pelo risco (por exemplo, **Sharpe Ratio**)\n",
    "- Lucro médio com penalizações por drawdown ou volatilidade\n",
    "\n",
    "Estas opções são mencionadas como possíveis objetivos de aprendizado por reforço, mas **os autores escolhem usar o TP (lucro total)** como função objetivo principal ao longo do artigo, por simplificar a explicação e facilitar a implementação.\n",
    "\n",
    "> Essas abordagens alternativas serão discutidas mais detalhadamente na Seção V do artigo.\n",
    "\n",
    "### Continuação: Aprendizado Direto da Política\n",
    "\n",
    "Com a função de recompensa já bem definida, o desafio agora é:  \n",
    "**como otimizar essa função de forma eficiente**?\n",
    "\n",
    "---\n",
    "\n",
    "#### Limitações do Aprendizado Convencional por Reforço\n",
    "\n",
    "Nos métodos clássicos de *Reinforcement Learning*, as funções de valor são definidas em **espaços discretos de estados** e otimizadas via **programação dinâmica** (como Q-Learning ou métodos de política-valor).\n",
    "\n",
    "No entanto, os autores destacam que essa abordagem **não funciona bem para negociação financeira**, porque:\n",
    "\n",
    "- O mercado é altamente **complexo e contínuo**\n",
    "- É difícil representar as condições de mercado com um número razoável de **estados discretos**\n",
    "- O ambiente é **não estacionário** e com ruídos, o que dificulta aprender o valor de cada estado\n",
    "\n",
    "---\n",
    "\n",
    "#### A Contribuição do DRL\n",
    "\n",
    "Diante dessas limitações, a proposta de {cite:p}`Moody01` e aprofundada neste artigo é:\n",
    "\n",
    "> Em vez de aprender o valor de cada estado, **aprender diretamente a política** de decisão.\n",
    "\n",
    "Esse paradigma é chamado de **Aprendizado por Reforço Direto** (*Direct Reinforcement Learning – DRL*).\n",
    "\n",
    "---\n",
    "\n",
    "#### Função da Política Direta (Equação 3)\n",
    "\n",
    "A política de negociação é representada por uma **função não linear**, que estima diretamente a ação a ser tomada no tempo $ t $:\n",
    "\n",
    "$$\n",
    "\\delta_t = \\tanh\\left( \\langle \\mathbf{w}, \\mathbf{f}_t \\rangle + b + u \\cdot \\delta_{t-1} \\right)\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ \\delta_t \\in (-1, 1) $: saída contínua da política, representando a intensidade da posição (tanh aproxima os sinais $-1$, $0$, $1$)\n",
    "- $ \\mathbf{f}_t $: vetor de **características do mercado** no tempo $ t $ (ex: indicadores, retornos, volatilidade etc.)\n",
    "- $ \\mathbf{w} $: vetor de **pesos (parâmetros)** aprendidos para cada feature\n",
    "- $ b $: **viés (bias)** da função\n",
    "- $ u \\cdot \\delta_{t-1} $: termo de **memória**, que introduz dependência da ação anterior\n",
    "- $ \\langle \\mathbf{w}, \\mathbf{f}_t \\rangle $: **produto interno** (ou somatório ponderado das features)\n",
    "\n",
    "A função tangente hiperbólica $ \\tanh(\\cdot) $ comprime a saída para o intervalo $ (-1, 1) $, permitindo representar tanto posições compradas, neutras ou vendidas **de forma contínua**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretação\n",
    "\n",
    "Esse modelo representa uma **rede neural muito simples**, com:\n",
    "\n",
    "- Uma camada linear: $ \\langle \\mathbf{w}, \\mathbf{f}_t \\rangle + b $\n",
    "- Um **feedback da decisão anterior** via $ u \\cdot \\delta_{t-1} $\n",
    "- Uma ativação $ \\tanh $: que modela não só a direção (long, short, neutral), mas também a **intensidade** da decisão\n",
    "\n",
    "---\n",
    "\n",
    "> Esta é uma abordagem elegante, pois evita a discretização dos estados e permite que o modelo aprenda diretamente como agir a partir dos dados brutos e históricos do mercado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ccbab1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
