{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bad3ea",
   "metadata": {},
   "source": [
    "##  Task-Aware BPTT (Retropropagação no Tempo Sensível à Tarefa)\n",
    "\n",
    "###  Contexto do Problema\n",
    "\n",
    "Após a inicialização dos parâmetros (Seção IV.A), o próximo passo é o **ajuste fino** (fine tuning) do modelo FDRNN.  \n",
    "Esse ajuste exige a aplicação do **método de retropropagação do erro**, mas com uma dificuldade adicional:\n",
    "\n",
    "> A arquitetura FDRNN é **profunda e recorrente ao mesmo tempo**, o que torna o cálculo dos gradientes mais complexo do que em redes feedforward simples.\n",
    "\n",
    "---\n",
    "\n",
    "###  Gradiente com estrutura recorrente\n",
    "\n",
    "Os autores indicam que, devido à recorrência, o cálculo do gradiente para qualquer parâmetro **envolve múltiplos termos recursivos**, incluindo dependências de estados anteriores ($δ_{t−1}$, $δ_{t−2}$, ..., até o tempo inicial).  \n",
    "\n",
    "O gradiente da função de recompensa total $U_T$ com relação a um parâmetro $θ$ segue uma cadeia de dependência:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4091b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial U_T}{\\partial \\theta} = \\sum_t \\frac{d U_t}{d R_t} \\left\\{ \n",
    "\\frac{d R_t}{d \\delta_t} \\cdot \\frac{d \\delta_t}{d \\theta} \n",
    "+ \\frac{d R_t}{d \\delta_{t-1}} \\cdot \\frac{d \\delta_{t-1}}{d \\theta} \n",
    "\\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d \\delta_t}{d \\theta} = \\frac{\\partial \\delta_t}{\\partial \\theta} \n",
    "+ \\frac{\\partial \\delta_t}{\\partial \\delta_{t-1}} \\cdot \\frac{d \\delta_{t-1}}{d \\theta}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551b2b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Isso gera uma **explosão de termos recursivos**, que torna o cálculo direto inviável e propenso a instabilidade.\n",
    "\n",
    "---\n",
    "\n",
    "###  Solução: Backpropagation Through Time (BPTT)\n",
    "\n",
    "Para resolver isso, o artigo propõe usar a técnica conhecida como **Backpropagation Through Time (BPTT)**:\n",
    "\n",
    "- O BPTT **\"desenrola\" a rede no tempo**, transformando a estrutura recorrente em uma sequência de redes feedforward.\n",
    "- Cada \"tempo\" se torna uma **cópia da rede**, chamada de **time stack**.\n",
    "- Com isso, a rede **perde sua recorrência temporariamente**, o que permite aplicar o algoritmo de backpropagation comum.\n",
    "\n",
    "#### Exemplo visual:\n",
    "- `τ = 0` → estado atual\n",
    "- `τ = 1` → um passo anterior\n",
    "- Cada `τ` é uma cópia da rede com seus próprios estados e camadas ocultas\n",
    "\n",
    "---\n",
    "\n",
    "###  Problema: Degradação do gradiente\n",
    "\n",
    "Ao aplicar BPTT diretamente, surgem dois problemas:\n",
    "\n",
    "1. A rede **fica ainda mais profunda**, já que se estende ao longo do tempo (tempo × profundidade = profundidade total maior).\n",
    "2. O gradiente **tende a desaparecer** (vanishing gradient), especialmente:\n",
    "   - Em camadas mais próximas da entrada\n",
    "   - Em time stacks mais antigos (com `τ` alto)\n",
    "\n",
    "Isso **prejudica o aprendizado**, porque os sinais de erro não chegam adequadamente às camadas que mais precisam ser ajustadas.\n",
    "\n",
    "---\n",
    "\n",
    "###  Proposta: Task-Aware BPTT\n",
    "\n",
    "Para resolver esse problema, os autores propõem o **Task-Aware BPTT**:\n",
    "\n",
    "- Durante o desenrolar no tempo, **linhas virtuais** são adicionadas, conectando diretamente:\n",
    "  1. A **função objetivo `U_T`** com a saída de cada time stack\n",
    "  2. A **saída de cada time stack** com suas respectivas camadas ocultas\n",
    "\n",
    "#### O que isso muda?\n",
    "\n",
    "- Cada bloco temporal **recebe gradientes diretamente da tarefa final** (`U_T`), além dos gradientes das etapas anteriores.\n",
    "- Isso **reforça a retropropagação** e **reduz a perda de sinal de erro**, especialmente nas camadas mais distantes e nos tempos mais antigos.\n",
    "\n",
    "---\n",
    "\n",
    "###  Resumo da técnica\n",
    "\n",
    "| Conceito                 | Explicação                                                             |\n",
    "|--------------------------|------------------------------------------------------------------------|\n",
    "| Time stack               | Cópia da rede para um instante no tempo (`τ`)                         |\n",
    "| BPTT                     | Desenrola a recorrência para aplicar backpropagation padrão           |\n",
    "| Problema do gradiente    | Em redes profundas e longas, o erro \"some\" antes de alcançar as bases |\n",
    "| Task-aware BPTT          | Injeta gradiente diretamente da função de recompensa em cada tempo     |\n",
    "| Benefício                | Aprendizado mais eficaz em todas as camadas e tempos                  |\n",
    "\n",
    "---\n",
    "\n",
    "###  Algoritmo 1 (resumo)\n",
    "\n",
    "O algoritmo descrito no final da seção segue os seguintes passos:\n",
    "\n",
    "1. Inicialize os parâmetros da FDRNN (já feito na Seção IV.A).\n",
    "2. Desenrole a rede no tempo para formar os time stacks.\n",
    "3. Calcule os gradientes em cada time stack, com:\n",
    "   - Gradientes normais (da retropropagação padrão)\n",
    "   - Gradientes adicionais vindos da função `U_T`\n",
    "4. **Normalize o vetor de gradiente** (para evitar explosões numéricas).\n",
    "5. Atualize os parâmetros com descida de gradiente.\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusão\n",
    "\n",
    "O **Task-Aware BPTT** é uma adaptação sofisticada do BPTT tradicional, ajustada às necessidades de:\n",
    "\n",
    "- Redes **profundas e recorrentes**\n",
    "- Treinamento com **função de recompensa não supervisionada**\n",
    "- Aplicações **financeiras ruidosas e sensíveis ao tempo**\n",
    "\n",
    "Essa técnica permite **treinar a FDRNN de forma robusta**, mesmo com grande profundidade e dependência temporal.\n",
    "\n",
    "Na próxima seção, o artigo testa esse modelo no mundo real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
