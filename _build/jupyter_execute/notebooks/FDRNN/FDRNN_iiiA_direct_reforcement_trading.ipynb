{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6e2165",
   "metadata": {},
   "source": [
    "# Direct Deep Reinforcement Learning\n",
    "\n",
    "## A. Negociação por Reforço Direto (Direct Reinforcement Trading)\n",
    "\n",
    "Nesta seção, os autores revisam a estrutura de *aprendizado por reforço direto* (DRL - *Direct Reinforcement Learning*), originalmente proposta por {cite:p}`Moody98`. Eles mostram que esse tipo de DRL pode ser interpretado como uma **rede neural recorrente (RNN)** com apenas **uma camada**, o que a torna um modelo relativamente simples, mas ainda assim útil.\n",
    "\n",
    "O ambiente é modelado como uma sequência de preços de um ativo financeiro ao longo do tempo:\n",
    "\n",
    "$$\n",
    "p_1, p_2, \\dots, p_t, \\dots\n",
    "$$\n",
    "\n",
    "A variação de preço (ou retorno) entre dois instantes consecutivos é definida por:\n",
    "\n",
    "$$\n",
    "z_t = p_t - p_{t-1}\n",
    "$$\n",
    "\n",
    "Com base nas condições atuais do mercado, o modelo toma uma **decisão de negociação em tempo real** a cada instante de tempo $t$. Essa decisão, chamada **$\\delta_t$**, pode assumir os seguintes valores:\n",
    "\n",
    "- $1$: posição comprada (*long*)\n",
    "- $0$: posição neutra (*neutral*)\n",
    "- $-1$: posição vendida (*short*)\n",
    "\n",
    "O **lucro (ou prejuízo) no tempo $t$**, representado por $R_t$, é dado pela seguinte equação:\n",
    "\n",
    "$$\n",
    "R_t = \\delta_{t-1} \\cdot z_t - c \\cdot \\left| \\delta_t - \\delta_{t-1} \\right|\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "### Interpretação da fórmula:\n",
    "\n",
    "- O primeiro termo $\\delta_{t-1} \\cdot z_t$ representa o ganho (ou perda) obtido com a variação do mercado, **de acordo com a posição assumida no período anterior**.\n",
    "  - Exemplo: se você estava comprado ($\\delta_{t-1} = 1$) e o preço subiu ($z_t > 0$), você tem lucro.\n",
    "  - Se estava vendido ($\\delta_{t-1} = -1$) e o preço caiu ($z_t < 0$), também há lucro.\n",
    "  - Se estava neutro ($\\delta_{t-1} = 0$), esse termo é nulo.\n",
    "\n",
    "- O segundo termo $c \\cdot \\left| \\delta_t - \\delta_{t-1} \\right|$ representa o **custo de transação (transaction cost - TC)**. Ele é cobrado **apenas quando há mudança de posição** entre dois instantes consecutivos.\n",
    "  - Exemplo: mudar de neutro para comprado gera custo.\n",
    "  - Manter a mesma posição ($\\delta_t = \\delta_{t-1}$) **não gera custo**.\n",
    "\n",
    "O parâmetro $c$ representa esse custo, que pode incluir taxas da corretora, impostos, ou perdas por slippage (diferença entre o preço esperado e o executado). Esses custos são importantes para simular de forma realista os impactos de decisões frequentes de compra e venda no lucro final.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f4d98",
   "metadata": {},
   "source": [
    "## Exemplos de Ganho e Perda com a Fórmula de Reforço Direto\n",
    "\n",
    "A fórmula do lucro/prejuízo a cada instante de tempo $ t $ é:\n",
    "\n",
    "$$\n",
    "R_t = \\delta_{t-1} \\cdot z_t - c \\cdot |\\delta_t - \\delta_{t-1}|\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ \\delta_t \\in \\{-1, 0, 1\\} $: posição assumida (vendida, neutra ou comprada)\n",
    "- $ z_t = p_t - p_{t-1} $: variação de preço no período\n",
    "- $ c $: custo de transação (por exemplo, $ c = 0.5 $)\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 1: Lucro com posição comprada\n",
    "\n",
    "- $ p_{t-1} = 100 $\n",
    "- $ p_t = 103 $ → $ z_t = 3 $\n",
    "- $ \\delta_{t-1} = 1 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = 1 \\cdot 3 - 0.5 \\cdot |1 - 1| = 3 - 0 = \\boxed{3}\n",
    "$$\n",
    "\n",
    "**Lucro de 3 unidades**.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 2: Prejuízo com posição comprada\n",
    "\n",
    "- $ z_t = -2 $ (queda de preço)\n",
    "- $ \\delta_{t-1} = 1 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = 1 \\cdot (-2) - 0.5 \\cdot |1 - 1| = -2 - 0 = \\boxed{-2}\n",
    "$$\n",
    "\n",
    "**Prejuízo de 2 unidades**, sem custo de transação.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 3: Lucro com posição vendida\n",
    "\n",
    "- $ z_t = -4 $\n",
    "- $ \\delta_{t-1} = -1 $\n",
    "- $ \\delta_t = -1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = (-1) \\cdot (-4) - 0.5 \\cdot |-1 - (-1)| = 4 - 0 = \\boxed{4}\n",
    "$$\n",
    "\n",
    "**Lucro de 4 unidades**, pois o preço caiu e o agente estava vendido.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 4: Mudança de posição com custo, sem retorno imediato\n",
    "\n",
    "- $ z_t = 2 $\n",
    "- $ \\delta_{t-1} = 0 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = 0 \\cdot 2 - 0.5 \\cdot |1 - 0| = 0 - 0.5 = \\boxed{-0.5}\n",
    "$$\n",
    "\n",
    "***Perda de 0.5 unidades**, referente apenas ao custo de entrada.\n",
    "\n",
    "---\n",
    "\n",
    "### Exemplo 5: Mudança de posição com lucro subsequente\n",
    "\n",
    "#### No tempo $ t $:\n",
    "\n",
    "- $ z_t = 5 $\n",
    "- $ \\delta_{t-1} = 0 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ c = 0.5 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_t = 0 \\cdot 5 - 0.5 \\cdot |1 - 0| = 0 - 0.5 = \\boxed{-0.5}\n",
    "$$\n",
    "\n",
    "#### No tempo $ t+1 $:\n",
    "\n",
    "- $ z_{t+1} = 3 $\n",
    "- $ \\delta_t = 1 $\n",
    "- $ \\delta_{t+1} = 1 $\n",
    "\n",
    "**Cálculo:**\n",
    "\n",
    "$$\n",
    "R_{t+1} = 1 \\cdot 3 - 0.5 \\cdot |1 - 1| = 3 - 0 = \\boxed{3}\n",
    "$$\n",
    "\n",
    "**Resumo**: perdeu 0.5 na entrada, mas ganhou 3 depois → **Lucro líquido: 2.5**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8545dd",
   "metadata": {},
   "source": [
    "A função apresentada anteriormente (Equação 1) representa a **função de valor** típica usada em frameworks de *Deep Reinforcement Learning* (DRL). Essa função avalia o **lucro instantâneo** $ R_t $ em cada ponto do tempo.\n",
    "\n",
    "Com base nesses valores $ R_t $, é possível definir o valor acumulado ao longo de todo o período de treinamento como:\n",
    "\n",
    "$$\n",
    "\\max_{\\Theta} \\ U_T\\{R_1, R_2, \\dots, R_T \\ | \\ \\Theta\\}\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ U_T\\{\\cdot\\} $ é a função que **acumula os lucros/recompensas** entre os tempos $ t = 1 $ até $ T $\n",
    "- $ \\Theta $ é a política de negociação, ou seja, a estratégia usada para tomar decisões ao longo do tempo\n",
    "- O objetivo do treinamento é encontrar a política $ \\Theta $ que **maximiza** esse valor acumulado\n",
    "\n",
    "#### Recompensa acumulada mais simples: Total Profit (TP)\n",
    "\n",
    "A forma mais direta de acumular as recompensas é simplesmente somar os lucros em todos os períodos:\n",
    "\n",
    "$$\n",
    "U_T = \\sum_{t=1}^{T} R_t\n",
    "$$\n",
    "\n",
    "Essa soma representa o **lucro total (Total Profit – TP)** obtido no período completo de treinamento.\n",
    "\n",
    "#### Alternativas mais sofisticadas\n",
    "\n",
    "Embora o lucro total seja a forma mais intuitiva de recompensa, é possível utilizar **funções de recompensa mais complexas**, como:\n",
    "\n",
    "- Retorno ajustado pelo risco (por exemplo, **Sharpe Ratio**)\n",
    "- Lucro médio com penalizações por drawdown ou volatilidade\n",
    "\n",
    "Estas opções são mencionadas como possíveis objetivos de aprendizado por reforço, mas **os autores escolhem usar o TP (lucro total)** como função objetivo principal ao longo do artigo, por simplificar a explicação e facilitar a implementação.\n",
    "\n",
    "> Essas abordagens alternativas serão discutidas mais detalhadamente na Seção V do artigo.\n",
    "\n",
    "### Aprendizado Direto da Política\n",
    "\n",
    "Com a função de recompensa já bem definida, o desafio agora é:  **como otimizar essa função de forma eficiente**?\n",
    "\n",
    "---\n",
    "\n",
    "#### Limitações do Aprendizado Convencional por Reforço\n",
    "\n",
    "Nos métodos clássicos de *Reinforcement Learning*, as funções de valor são definidas em **espaços discretos de estados** e otimizadas via **programação dinâmica** (como Q-Learning ou métodos de política-valor).\n",
    "\n",
    "No entanto, os autores destacam que essa abordagem **não funciona bem para negociação financeira**, porque:\n",
    "\n",
    "- O mercado é altamente **complexo e contínuo**\n",
    "- É difícil representar as condições de mercado com um número razoável de **estados discretos**\n",
    "- O ambiente é **não estacionário** e com ruídos, o que dificulta aprender o valor de cada estado\n",
    "\n",
    "---\n",
    "\n",
    "#### A Contribuição do DRL\n",
    "\n",
    "Diante dessas limitações, a proposta de {cite:p}`Moody01` e aprofundada no artigo é:\n",
    "\n",
    "> Em vez de aprender o valor de cada estado, **aprender diretamente a política** de decisão.\n",
    "\n",
    "Esse paradigma é chamado de **Aprendizado por Reforço Direto** (*Direct Reinforcement Learning – DRL*).\n",
    "\n",
    "---\n",
    "\n",
    "#### Função da Política Direta \n",
    "\n",
    "A política de negociação é representada por uma **função não linear**, que estima diretamente a ação a ser tomada no tempo $ t $:\n",
    "\n",
    "$$\n",
    "\\delta_t = \\tanh\\left( \\langle \\mathbf{w}, \\mathbf{f}_t \\rangle + b + u \\cdot \\delta_{t-1} \\right)\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ \\delta_t \\in (-1, 1) $: saída contínua da política, representando a intensidade da posição (tanh aproxima os sinais $-1$, $0$, $1$)\n",
    "- $ \\mathbf{f}_t $: vetor de **features do mercado** no tempo $ t $ (ex: indicadores, retornos, volatilidade etc.)\n",
    "- $ \\mathbf{w} $: vetor de **pesos (parâmetros)** aprendidos para cada feature\n",
    "- $ b $: **viés (bias)** da função\n",
    "- $ u \\cdot \\delta_{t-1} $: termo de **memória**, que introduz dependência da ação anterior\n",
    "- $ \\langle \\mathbf{w}, \\mathbf{f}_t \\rangle $: **produto interno** (ou somatório ponderado das features)\n",
    "\n",
    "A função tangente hiperbólica $ \\tanh(\\cdot) $ comprime a saída para o intervalo $ (-1, 1) $, permitindo representar tanto posições compradas, neutras ou vendidas **de forma contínua**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretação\n",
    " \n",
    "O modelo base de decisão do paper é uma rede neural extremamente simples, mas poderosa. Ela é composta por três partes principais:\n",
    "\n",
    "---\n",
    "\n",
    "1. Camada Linear\n",
    "\n",
    "A primeira etapa da rede é uma **soma ponderada das entradas do mercado**, também chamada de produto interno.\n",
    "\n",
    "- Cada feature do vetor de entrada tem um peso associado.\n",
    "- Também há um termo de viés (bias), que serve como um deslocamento.\n",
    "- Essa combinação resulta em um valor escalar que representa a **situação atual do mercado** conforme interpretada pela rede.\n",
    "\n",
    "**Intuição:**  \n",
    "É como um trader que diz:  \n",
    "“Dou mais importância aos retornos recentes do que aos antigos, e tendo a favorecer posições compradas em geral.”\n",
    "\n",
    "---\n",
    "\n",
    "2. Feedback da Ação Anterior\n",
    "\n",
    "O modelo incorpora um **termo de memória**, que é a **posição de trading tomada no tempo anterior multiplicada por um peso**.\n",
    "\n",
    "- Isso permite ao modelo **levar em conta sua decisão passada** ao decidir o que fazer agora.\n",
    "- A ideia é evitar trocas desnecessárias de posição, o que reduz custos operacionais (como taxas e slippage).\n",
    "- Também permite que o modelo **favoreça a continuidade das ações** quando o mercado não muda drasticamente.\n",
    "\n",
    "**Intuição:**  \n",
    "É como um trader que pensa:  \n",
    "“Se eu estava comprado ontem e não mudou muita coisa, talvez seja melhor continuar comprado.”\n",
    "\n",
    "---\n",
    "\n",
    "3. Ativação Tanh: Direção e Intensidade\n",
    "\n",
    "Após a soma ponderada e o termo de memória, o resultado passa por uma **função de ativação tangente hiperbólica**.\n",
    "\n",
    "- Essa função gera um valor contínuo entre -1 e 1.\n",
    "- O resultado já é a **ação de trading**:\n",
    "  - Valores próximos de -1 indicam posição vendida.\n",
    "  - Valores próximos de 0 indicam posição neutra.\n",
    "  - Valores próximos de 1 indicam posição comprada.\n",
    "- Isso também permite **posições parciais**, como meio comprado ou meio vendido.\n",
    "\n",
    "**Vantagem:**  \n",
    "Não é necessário discretizar ações (como \"comprar\", \"vender\", \"parar\").  \n",
    "O modelo já produz ações contínuas, mais suaves e mais fáceis de otimizar via gradiente.\n",
    "\n",
    "---\n",
    "\n",
    "Por que isso é elegante?\n",
    "\n",
    "Com uma estrutura extremamente simples, o modelo já consegue:\n",
    "\n",
    "- Ter **memória** do passado (recurrente).\n",
    "- Fazer decisões **suaves e contínuas**.\n",
    "- Aprender **diretamente dos dados brutos do mercado**.\n",
    "- **Evitar regras manuais** ou discretização de estados/ações.\n",
    "\n",
    "É uma forma poderosa e minimalista de tomar decisões em tempo real no mercado financeiro.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ccbab1",
   "metadata": {},
   "source": [
    "## Equação para $\\bf f_t$  e a Estrutura do DRL\n",
    "\n",
    "No modelo de **Reinforcement Learning Direto (DRL)**, os **m retornos mais recentes** são utilizados diretamente como vetor de entrada (features), representado por:\n",
    "$$\n",
    "{\\bf f_t} = [z_{t-m+1}, ..., z_t] ∈ ℝ^m\n",
    "$$\n",
    "\n",
    "\n",
    "Além dessas features, um **termo adicional** $u * δ_{t-1}$ também é incluído na regressão. Esse termo representa a **decisão de trading do instante anterior** e é incorporado ao modelo para **desencorajar mudanças frequentes de posição**, com o objetivo de **evitar custos de transação elevados**.\n",
    "\n",
    "A saída dessa soma linear é então passada por uma função de ativação **tangente hiperbólica (tanh)**, que mapeia o resultado para o intervalo **(-1, 1)**. Esse valor final é interpretado como a **decisão de trading do agente** naquele momento, podendo ser:\n",
    "\n",
    "- Próximo de -1 → posição vendida\n",
    "- Próximo de  0 → posição neutra\n",
    "- Próximo de +1 → posição comprada\n",
    "\n",
    "A **otimização do DRL** consiste em encontrar o melhor conjunto de parâmetros:\n",
    "$$\n",
    "Θ = {{\\bf w}, u, b}\n",
    "$$\n",
    "que **maximiza a função de recompensa global** definida anteriormente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3546a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}