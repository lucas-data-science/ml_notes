{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bad3ea",
   "metadata": {},
   "source": [
    "# DRNN LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b71c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Desafios no Treinamento da FDRNN\n",
    "\n",
    "###  Problema apontado\n",
    "\n",
    "Embora a formulação apresentada na equação (8) seja **conceitualmente elegante**, ela traz uma **grande dificuldade prática**:\n",
    "\n",
    "> A otimização do modelo se torna **muito complexa**, pois envolve uma rede neural profunda com **milhares de parâmetros ocultos (latentes)**.\n",
    "\n",
    "Em outras palavras:\n",
    "- O modelo FDRNN combina três componentes:\n",
    "  - Representação fuzzy;\n",
    "  - Transformação profunda (Deep Neural Network);\n",
    "  - Política de decisão recorrente.\n",
    "- Isso resulta em uma arquitetura grande e interdependente.\n",
    "- Otimizar tudo isso de uma vez, diretamente, é computacionalmente difícil e propenso a instabilidade.\n",
    "\n",
    "---\n",
    "\n",
    "### Solução proposta\n",
    "\n",
    "Para contornar essa dificuldade, os autores propõem uma **estratégia de treinamento prática** baseada em **duas etapas principais**:\n",
    "\n",
    "1. **Inicialização do sistema (System Initialization)**\n",
    "   - Treinar separadamente partes da rede para obter parâmetros razoáveis antes do treinamento conjunto.\n",
    "\n",
    "2. **Ajuste fino (Fine Tuning)**\n",
    "   - Refinar todos os parâmetros juntos de forma mais precisa e específica para a tarefa final, após uma boa inicialização.\n",
    "\n",
    "Essa abordagem divide o problema em etapas mais gerenciáveis, facilitando o treinamento da rede como um todo.\n",
    "\n",
    "---\n",
    "\n",
    "###  Intenção dessa seção\n",
    "\n",
    "A partir daqui, a **Seção IV** vai detalhar **como implementar esse treinamento em duas fases**, explicando como lidar com a recorrência, com os gradientes temporais, e como evitar problemas como o desaparecimento ou explosão do gradiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd955d66",
   "metadata": {},
   "source": [
    "##  Inicialização do Sistema\n",
    "\n",
    "A inicialização dos parâmetros é **uma etapa crítica** no treinamento de redes neurais profundas (DNNs), especialmente quando se trabalha com estruturas complexas como o FDRNN. Esta seção descreve como inicializar **cada uma das três partes do modelo**:  \n",
    "1. Representação Fuzzy  \n",
    "2. Transformação Profunda (Deep Learning)  \n",
    "3. Política de Trading (DRL)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.  Inicialização da Representação Fuzzy\n",
    "\n",
    "- Cada entrada do vetor ${\\bf f}_t$ é conectada a **três funções de pertinência fuzzy**.\n",
    "- Cada função fuzzy tem dois parâmetros:\n",
    "  - Centro $m_i$\n",
    "  - Largura (variância) $σ_i²$\n",
    "\n",
    "#### Estratégia:\n",
    "- Utiliza-se o algoritmo **k-means** para dividir os dados de treino em **k = 3 grupos**:\n",
    "  - Correspondentes às três categorias: tendência de alta, baixa e neutra.\n",
    "- Em cada cluster, calcula-se a **média e a variância** de cada dimensão do vetor ${\\bf f}_t$.\n",
    "- Esses valores são usados como inicialização dos **centros e larguras das funções fuzzy**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.  Inicialização da Transformação Profunda (Rede Densa)\n",
    "\n",
    "Para inicializar a rede profunda, os autores utilizam uma técnica clássica de **aprendizado não supervisionado** chamada **Autoencoder (AE)**.\n",
    "\n",
    "#### O que é um Autoencoder?\n",
    "- Um AE tenta **reconstruir a entrada** após passá-la por uma ou mais camadas ocultas.\n",
    "- Serve para aprender **representações úteis (codificações)** sem usar rótulos.\n",
    "\n",
    "#### Como é usado aqui:\n",
    "- Define-se três camadas:\n",
    "  - Camada de entrada ($l$)\n",
    "  - Camada oculta ($l + 1$)\n",
    "  - Camada de reconstrução ($l + 2$)\n",
    "- Aplica-se uma transformação $h_θ(·)$ da camada $l$ para a $l+1$ e $h_γ(·)$ da $l+1$ para a $l+2$.\n",
    "\n",
    "#### Função de perda do AE:\n",
    "A minimização é feita sobre:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d592e",
   "metadata": {},
   "source": [
    "$$Loss = || {\\bf x}_t − h_γ(h_θ({\\bf x}_t)) ||² + η * || {\\bf w}_(l+1) ||²$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0a57d",
   "metadata": {},
   "source": [
    "\n",
    "- O primeiro termo mede o erro de reconstrução (diferença entre entrada e saída).\n",
    "- O segundo termo é uma **penalização de regularização (L2)** para evitar overfitting.\n",
    "\n",
    "#### Resultado:\n",
    "- Após treinar o AE, os **parâmetros da camada oculta $l+1$ (θ = {w, b})** são salvos e usados como inicialização da rede profunda.\n",
    "- Os parâmetros da camada de reconstrução ($γ$) **são descartados** — ela é apenas uma estrutura auxiliar.\n",
    "\n",
    "#### Processo:\n",
    "- Esse procedimento é **repetido camada por camada**, até que todas as camadas da rede profunda estejam inicializadas.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.  Inicialização da Parte de Decisão (DRL)\n",
    "\n",
    "- Após obter a representação profunda $F_t$ com o DNN inicializado, usa-se essa entrada para **treinar a parte de decisão** ($δ_t$).\n",
    "- Isso equivale a treinar uma **rede rasa recorrente** (como na Fig. 1(a)), já conhecida da literatura.\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusão da Inicialização\n",
    "\n",
    "Todos os procedimentos descritos até aqui tratam apenas de **inicializações**.  \n",
    "No entanto, para que o sistema como um todo seja eficaz, ainda é necessário um **ajuste fino (fine tuning)**.\n",
    "\n",
    "Esse ajuste final:\n",
    "- Ajusta os parâmetros de **todas as partes da rede**.\n",
    "- É feito de forma **sensível à tarefa final** (no caso, maximizar retorno).\n",
    "- Pode ser visto como um processo de **aprendizado de representações dependente da tarefa** (task-dependent feature learning).\n",
    "\n",
    "A próxima seção abordará justamente esse passo: **como treinar a rede recorrente profunda com backpropagation através do tempo (BPTT)** de maneira adequada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9a114",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
